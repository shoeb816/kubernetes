# Set up a Highly Available Kubernetes Cluster using kubeadm
Follow this documentation to set up a highly available Kubernetes cluster using __Ubuntu 22.04 LTS__ with keepalived and haproxy

This documentation guides you in setting up a cluster with three master nodes, one worker node and two load balancer node using HAProxy and Keepalived.

## Environment
|Server |FQDN|IP|OS|RAM|CPU|
|----|----|----|----|----|----|
|Load Balancer|klb01.kpp.com|192.168.1.211|1024M|1|
|Load Balancer|klb02.kpp.com|192.168.1.212|1024M|1|
|Master|kmaster01.kpp.com|192.168.1.204|Ubuntu 20.04|2G|2|
|Master|kmaster02.kpp.com|192.168.1.205|Ubuntu 20.04|2G|2|
|Master|kmaster03.kpp.com|192.168.1.206|Ubuntu 20.04|2G|2|
|Worker|kworker1.kpp.com|192.168.1.210|Ubuntu 20.04|2G|2|

|Virtual IP|
|----|
|192.168.1.213|

## Pre-requisites
If you want to try this in a virtualized environment on your workstation
* VMware/Virtualbox installed
* Vagrant installed and configure (Check this ---> https://github.com/shoeb816/kubernetes_vagrant.git)
* Host machine has atleast 12 cores
* Host machine has atleast 16G memory


####################Basic Configuration
sudo cat <<EOF>> /etc/hosts 
192.168.1.204 kmaster01 kmaster01.kpp.com
192.168.1.205 kmaster02 kmaster02.kpp.com
192.168.1.206 kmaster03 kmaster03.kpp.com
192.168.1.211 klb01     klb01.kpp.com
192.168.1.212 klb02     klb02.kpp.com
192.168.1.213 kvip      kvip.kpp.com
192.168.1.214 knode01   knode01.kpp.com
EOF

echo "$USER ALL=(ALL:ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/$USER

####################Load Balancer Configuration
##### Install Keepalived & Haproxy

```
apt update && apt install -y keepalived haproxy
```

##### configure keepalived
On both nodes create the health check script /etc/keepalived/check_apiserver.sh

```
cat >> /etc/keepalived/check_apiserver.sh <<EOF
#!/bin/sh

errorExit() {
  echo "*** $@" 1>&2
  exit 1
}

curl --silent --max-time 2 --insecure https://localhost:6443/ -o /dev/null || errorExit "Error GET https://localhost:6443/"
if ip addr | grep -q 192.168.1.213; then
  curl --silent --max-time 2 --insecure https://192.168.1.213:6443/ -o /dev/null || errorExit "Error GET https://192.168.1.213:6443/"
fi
EOF

chmod +x /etc/keepalived/check_apiserver.sh
```

Create keepalived config /etc/keepalived/keepalived.conf

```
cat >> /etc/keepalived/keepalived.conf <<EOF
vrrp_script check_apiserver {
  script "/etc/keepalived/check_apiserver.sh"
  interval 3
  timeout 10
  fall 5
  rise 2
  weight -2
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 1
    priority 200
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass mysecret
    }
    virtual_ipaddress {
        192.168.1.213
    }
    track_script {
        check_apiserver
    }
}
EOF
```

##### Enable & start keepalived service

```
systemctl enable --now keepalived
```

##### Configure haproxy
Update **/etc/haproxy/haproxy.cfg**

```
cat >> /etc/haproxy/haproxy.cfg <<EOF

frontend kubernetes-frontend
  bind *:6443
  mode tcp
  option tcplog
  default_backend kubernetes-backend

backend kubernetes-backend
  option httpchk GET /healthz
  http-check expect status 200
  mode tcp
  option ssl-hello-chk
  balance roundrobin
    server kmaster01 192.168.1.204:6443 check fall 3 rise 2
    server kmaster02 192.168.1.205:6443 check fall 3 rise 2
    server kmaster03 192.168.1.206:6443 check fall 3 rise 2

EOF
```
##### Enable & restart haproxy service

```
systemctl enable haproxy && systemctl restart haproxy
```

####################Pre-requisites on all kubernetes nodes (Master & Workers)
##### Disable swap

```
swapoff -a; sed -i '/swap/d' /etc/fstab
```

##### Disable Firewall

```
systemctl disable --now ufw
```

##### Enable and Load Kernel modules

```
{
cat >> /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter
}
```

##### Add Kernel settings

```
{
cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system
}
```

##### Install containerd runtime

```
{
  apt update
  apt install -y containerd apt-transport-https
  containerd --version 
  mkdir /etc/containerd
  containerd config default > /etc/containerd/config.toml
  systemctl restart containerd
  systemctl enable containerd
}
```

##### Update containerd Version Base on Kubernetes Version and change to SystemdCgroup from false to true (Due to API-Server Restart)
|Kubernetes Version| containerd Version| CRI Version|
|----|----|----|
|1.24|1.7.0+, 1.6.4+|v1, v1alpha2|
|1.25|1.7.0+, 1.6.4+|v1, v1alpha2 **|
|1.26|1.7.0+, 1.6.15+|v1|

```
apt-mark unhold containerd
apt remove containerd -y


# kubernetes 1.26 requires at least 1.6
CONTAINERDVERSION=1.6.16-*
 
# add docker repo to apt (as docker distributes recent versions of containerd)
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
 
# install containerd and mark package for hold
apt-mark unhold containerd.io
apt update && apt install -y containerd.io=$CONTAINERDVERSION
apt-mark hold containerd.io
 
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml

vi /etc/containerd/config.toml
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
   [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true
			
systemctl restart containerd

```

##### Add apt repo for kubernetes

```
{
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
}
```

##### Install Kubernetes components

```
{
  apt update
  apt install -y kubeadm=1.26.3-00 kubelet=1.26.3-00 kubectl=1.26.3-00
}
```

## Bootstrap the cluster
## On KMASTER01
##### Initialize Kubernetes Cluster

```
kubeadm init --control-plane-endpoint="192.168.1.213:6443" --upload-certs --apiserver-advertise-address=192.168.1.204 --pod-network-cidr=172.16.0.0/16
```

Your Kubernetes control-plane has initialized successfully!

Copy the commands to join other master nodes and worker nodes.

##### Join Kubernetes Node to Cluster
Master
```
kubeadm join 192.168.1.213:6443 --token py9aow.g73lzdlaiere4wwc \
        --discovery-token-ca-cert-hash sha256:0a38174afb85399324d14124b8d3cd20f0bb3fac298dffd4ead6562e8e791a34 \
        --control-plane --certificate-key feed10c6b3c57f6802b190951d2928f42a0cb037972107987bda27f390d4a21d --apiserver-advertise-address=192.168.1.206 (205)
```

Worker
```
kubeadm join 192.168.1.213:6443 --token hd2e0i.1b0rdamgp1a4javx --discovery-token-ca-cert-hash sha256:0a38174afb85399324d14124b8d3cd20f0bb3fac298dffd4ead6562e8e791a34	
```

##### Deploy Calico network

```
kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.25/manifests/calico.yaml
```


## Downloading kube config to your local machine
On your host machine

```
mkdir ~/.kube
scp root@172.16.16.101:/etc/kubernetes/admin.conf ~/.kube/config
```

## Verifying the cluster
```
kubectl cluster-info
kubectl get nodes
```

Have Fun!!